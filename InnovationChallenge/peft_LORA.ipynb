{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Adapted from:\n",
    " https://www.kaggle.com/code/dassum/finetune-phi-2-on-custom-dataset/notebook"
   ],
   "id": "d0c62f4720d99eed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl",
   "id": "462c0857ae7c77c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ],
   "id": "93741a91a8084943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "e154ba94ef627e1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configure nvidia\n",
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ],
   "id": "749602f6f5e9b8c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_path = 'EN_Manually labeled.xlsx'\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "drop_under = 30\n",
    "small_groups = df[\"Topic\"].value_counts()[df[\"Topic\"].value_counts() < drop_under].index\n",
    "df = df[~df[\"Topic\"].isin(small_groups)]\n",
    "df[\"Topic\"].value_counts()"
   ],
   "id": "9cfb66df80d8fd6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "columns_of_interest = ['tweet', 'Topic']\n",
    "df_clean = df[columns_of_interest]\n",
    "df_clean = df_clean.astype({'tweet': 'string[pyarrow]', 'Topic': 'string[pyarrow]'})\n",
    "df_clean.index.name = 'id'\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_df, test_df = train_test_split(df_clean, test_size=0.2, random_state=seed, stratify=df_clean['Topic'],)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({'train': train_ds, 'test': test_ds})\n",
    "dataset"
   ],
   "id": "8adefcf85281cded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "device_map = {\"\": 0}"
   ],
   "id": "eb3a49e9534699fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name='microsoft/phi-2'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True)"
   ],
   "id": "156cd4a2bb5374a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print_gpu_utilization()"
   ],
   "id": "6bf8b461bb03027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def gen(model, p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ],
   "id": "990286eccdbdebb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['train'][index]['tweet']\n",
    "summary = dataset['train'][index]['Topic']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Categorize the following conversation as either Fact, Personal, Research, Weaponized, Public, Ridicule, Misuse, Risk, Advocacy, Prevention, Symptom, Treatment, Public Health, Unidentified, Simile, Diagnosis, Ads, Joke, Metaphor, Comparative.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ],
   "id": "1b6c1067172ef77b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from functools import partial\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Categorize the following conversation as either Fact, Personal, Research, Weaponized, Public, Ridicule, Misuse, Risk, Advocacy, Prevention, Symptom, Treatment, Public Health, Unidentified, Simile, Diagnosis, Ads, Joke, Metaphor, Comparative.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['tweet']}\" if sample[\"tweet\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['Topic']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'tweet', 'Topic'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ],
   "id": "43ed3300c39f3b58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length, seed, train_ds)\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length, seed, test_ds)\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(f\"Validation: {eval_dataset.shape}\")\n",
    "print(train_dataset)"
   ],
   "id": "f234662861f3dfd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ],
   "id": "ef0b892f303a7a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(original_model)",
   "id": "f280f028318badbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ],
   "id": "d1fd91daa52e6aa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(print_number_of_trainable_model_parameters(peft_model))",
   "id": "497c623472638c3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_dir = './training/checkpoints'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "peft_training_args.device"
   ],
   "id": "8c98db9d8ea7db7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "peft_trainer.train()",
   "id": "2ea8c45668720efe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Free memory for merging weights\n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "7c455dbaa56e4bdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True)\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ],
   "id": "d39f2c07e88c1aa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "check_point_load = './training/checkpoints/checkpoint-200'\n",
    "ft_model = PeftModel.from_pretrained(base_model, check_point_load,torch_dtype=torch.float16,is_trainable=False)\n",
    "ft_model.generation_config.pad_token_id = eval_tokenizer.pad_token_id"
   ],
   "id": "c0c6d4a06cd90800"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "dialogue = dataset['test'][index]['tweet']\n",
    "summary = dataset['test'][index]['Topic']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Categorize the following conversation as either Fact, Personal, Research, Weaponized, Public, Ridicule, Misuse, Risk, Advocacy, Prevention, Symptom, Treatment, Public Health, Unidentified, Simile, Diagnosis, Ads, Joke, Metaphor, Comparative.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "\n",
    "peft_model_res = gen(ft_model,formatted_prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('#End')\n",
    "final_output = prefix.split('\\n')[0]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{final_output}')"
   ],
   "id": "69c2ec1e9b039f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True)\n",
    "original_model.generation_config.pad_token_id = eval_tokenizer.pad_token_id"
   ],
   "id": "70846aad829fa973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_to_test = 200\n",
    "dialogues = dataset['test'][:num_to_test]['tweet']\n",
    "human_baseline_summaries = dataset['test'][:num_to_test]['Topic']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx in tqdm(range(num_to_test)):\n",
    "    dialogue = dialogues[idx]\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Categorize the following conversation as either Fact, Personal, Research, Weaponized, Public, Ridicule, Misuse, Risk, Advocacy, Prevention, Symptom, Treatment, Public Health, Unidentified, Simile, Diagnosis, Ads, Joke, Metaphor, Comparative.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    #print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    orig_final = original_model_text_output.split('\\n')[0]\n",
    "    peft_final = peft_model_text_output.split('\\n')[0]\n",
    "\n",
    "    original_model_summaries.append(orig_final)\n",
    "    peft_model_summaries.append(peft_final)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "df.head()"
   ],
   "id": "8825c3d15d8b2c94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install rouge_score",
   "id": "287413700de94a85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ],
   "id": "dfe8575ed3a49bec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ],
   "id": "4490f1c389841380"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
